{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5407c406",
   "metadata": {},
   "source": [
    "<a href=\"https://www.eventsforce.net/turingevents/frontend/reg/thome.csp?pageID=89551&eventID=249&traceRedir=2\"> <img src=\"images/turing.png\" alt=\"Header\" style=\"height: 200px;\" align=\"left\"/> </a> <a href=\"https://www.baskerville.ac.uk/\"> <img src=\"images/baskerville.png\" alt=\"Header\" style=\"height: 200px;\" /> </a> <a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\" align=\"right\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbc810",
   "metadata": {},
   "source": [
    "# Challenge Instructions\n",
    "## Baskerville â€” Accelerate your research with GPUs 2023 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22dc95",
   "metadata": {},
   "source": [
    "This challenge is based on part of the NVIDIA DLI course for Accelerating Data Engineering Pipelines. Please do not share this material publicly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257c630",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "- Read data into GPU memory using the `cuDF` library\n",
    "- Cleanse the data\n",
    "- Use the `Dask` library to read thousands of CSV files at once\n",
    "- Visualise data using the `Plotly` library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d394f",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "For this challenge, we will be using the US <a href=\"https://www.noaa.gov/\">NOAA's</a> Hourly Precipitation Data. It goes all the way back to 1940! We will be using an unmodified version of their data pulled straight from their database <a href=\"https://www.ncei.noaa.gov/data/coop-hourly-precipitation/v2/archive/\">here</a>, however I have provided a `fetch_data.sh` bash script to pull the data from the website into the `data` folder in your working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fea570",
   "metadata": {},
   "source": [
    "> **CHECK**\n",
    ">\n",
    "> Please run `fetch_data.sh` in a terminal, and check that your datasets are copied into your challenge folder by running the code below. This is a shell command that counts the number of CSV files contained in the `data` folder and should return the result `2010` (as of 2 Feb 2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdec452",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l data | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e326de",
   "metadata": {},
   "source": [
    "We highly recommend setting up a terminal on the side to monitor our GPUs with the following command:\n",
    "\n",
    "`watch -n0.1 nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8cad4c-d705-44b6-a205-aa866b13c5f9",
   "metadata": {},
   "source": [
    "### Configure your Baskerville environment\n",
    "\n",
    "There are a couple of environment variables to configure in order for the `Dask` and `Plotly` packages to work on Baskerville (may not be necessary on other HPC systems). Run the following to unset `$UCX_NET_DEVICES` and add the conda environment to `$PATH`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42caf43b-750d-4e02-84b5-a1bc9b952579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "del os.environ[\"UCX_NET_DEVICES\"]\n",
    "user = os.getenv('USER')\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/bask/projects/v/vjgo8416-training/users/{}/env_data_engineering/bin\".format(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9421355",
   "metadata": {},
   "source": [
    "### cuDF\n",
    "\n",
    "We can load the data into <a href=\"https://docs.rapids.ai/api/cudf/stable/\">cuDF</a> in order to run some analysis on it. The API for `cuDF` is very similar to the popular `Pandas` library. Let's the compare the two on a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69323ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read data into Pandas dataframe (CPU memory)\n",
    "df_cpu = pd.read_csv(\"data/AQC00914594.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c56bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read data into cuDF dataframe (GPU memory)\n",
    "df_gpu = cudf.read_csv(\"data/AQC00914594.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579f023a",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    ">\n",
    "> Why, in this case, is reading data into GPU memory slower than reading into CPU memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0d8ea",
   "metadata": {
    "tags": []
   },
   "source": [
    " A description of the raw dataset can be found in `data/readme.txt` or from their <a href=\"https://www.ncei.noaa.gov/data/coop-hourly-precipitation/v2/doc/readme.csv.txt\">website</a>. The key points are:\n",
    "\n",
    "* Each row represents a day's precipitation at a Hourly Precipitation Data (HPD) Station.\n",
    "* Precipitation is measured in hundredths of an inch\n",
    "* Each hour is represented as a column with HR00 being the first hour of the day and HR23 being the last.\n",
    "* A missing value is represented as -9999\n",
    "* The `DlySum` column is the daily sum across the hours in their respective rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd4861",
   "metadata": {},
   "source": [
    "#### `describe` method\n",
    "\n",
    "We can use the <a href=\"https://docs.rapids.ai/api/cudf/stable/api.html?highlight=describe#cudf.core.dataframe.DataFrame.describe\">`describe`</a> method in order to quickly catch unusual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpu.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9286b5",
   "metadata": {},
   "source": [
    "> **CHECK**\n",
    ">\n",
    "> * Where does the `-9999` show up in the summary?\n",
    "> * Besides the min, what other statistics does it impact?\n",
    "> * Any other unusual observations about the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416257e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "> **ANSWERS**\n",
    "> * The -9999 appears in the minimum. This should be a red flag to us because how is negative rainfall possible?\n",
    "> * This strongly affects the mean as the -9999 has a much larger magnitude than the other precipitation data\n",
    "> * It also strongly affects the standard deviation as it is based on the range between the minimum and maximum values\n",
    "> * For some columns like latitude and longitude, the standard deviation is `0`. That means all cells in a column have the same value. If we were only working with this .csv, this would be a waste of space, but this value changes depending on the .csv, so we'll keep it as is.\n",
    "> * Non-numerical columns will still have statistics computed for them, although the results don't have much meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f83ec",
   "metadata": {},
   "source": [
    "Thankfully, there is a straightforward way to fix this. As we are reading in the data, we can let `read_csv` know that we want to treat `-9999` as a missing value with the `na_values` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpu = pd.read_csv(\"data/AQC00914594.csv\", na_values='-9999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpu.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5a465",
   "metadata": {},
   "source": [
    "#### `usecols` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6756043",
   "metadata": {},
   "source": [
    "When working with large datasets, it's useful to specify which columns we want with the `usecols` parameter. The earlier we can filter our data, the less wasted computation we'll have.\n",
    "\n",
    "Another parameter we might consider is `dtype`. If we don't specify this, cuDF will sample a number of rows and infer the data type. This can cause issues with anomalous values. For instance, if we have a float while most of the values in a column are integers, cuDF will throw an error about a type mismatch when it finally finds and reads in the float. cuDF supports most <a href=\"https://numpy.org/doc/stable/user/basics.types.html\">NumPy Data Types</a> along with a few others.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effdb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpu = cudf.read_csv(\n",
    "    \"data/AQC00914594.csv\",\n",
    "    usecols=[\"STATION\", \"DlySum\", \"DATE\"],\n",
    "    dtype={\n",
    "        \"STATION\": \"object\",\n",
    "        \"DlySum\": np.uint32,\n",
    "        \"DATE\": \"datetime64[ns]\",\n",
    "    }\n",
    ")\n",
    "df_gpu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea65574",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb2c92",
   "metadata": {},
   "source": [
    "<a href=\"https://dask.org/\">Dask</a> is a library to parallelize Python libraries. Even better? It's device-agnostic, meaning that it scales on GPUs just as easily as it does with CPU computing. \n",
    "\n",
    "Dask has a replacement library called `dask_cudf` for <a href=\"https://docs.rapids.ai/api/cudf/stable/\">cuDF</a> with much of the same functions. It even uses the same <a href=\"https://docs.rapids.ai/api/cudf/stable/api.html?highlight=read_csv#cudf.io.csv.read_csv\">`read_csv`</a> function. Let's give it a try reading in all 2010 of our CSV files.\n",
    "\n",
    "Before we can use `dask_cudf`, we need to use [dask_cuda](https://docs.rapids.ai/api/dask-cuda/nightly/index.html) to distibute our GPU workloads across multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb2d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask_cuda.initialize import initialize\n",
    "import rmm\n",
    "\n",
    "visible_devices = [0,1] # maximum 2 GPUs per node\n",
    "temp_data_directory = '/tmp/' # define directory to buffer data\n",
    "device_memory_limit = 2**32 * .9 # use a fraction of 4GiB capacity to prevent memory errors\n",
    "\n",
    "cluster = LocalCUDACluster(\n",
    "    CUDA_VISIBLE_DEVICES=visible_devices,\n",
    "    local_directory=temp_data_directory,\n",
    "    rmm_pool_size=device_memory_limit,\n",
    "    # allows direct GPU-to-GPU data transfer\n",
    "    protocol='ucx', \n",
    "    enable_tcp_over_ucx=True,\n",
    "    enable_nvlink=True,\n",
    "    enable_infiniband=True,\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaffb28",
   "metadata": {},
   "source": [
    "#### Initializing Memory Pools\n",
    "\n",
    "Since allocating memory is often a performance bottleneck, it is usually a good idea [to initialize](https://docs.rapids.ai/api/rmm/stable/basics.html) a memory pool on each of our workers. When using a distributed cluster, we must use the `client.run` utility to make sure a function is executed on all available workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RMM pool on ALL workers\n",
    "def _rmm_pool():\n",
    "    rmm.reinitialize()\n",
    "client.run(_rmm_pool)# Initialize RMM pool on ALL workers\n",
    "def _rmm_pool():\n",
    "    rmm.reinitialize()\n",
    "client.run(_rmm_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import dask_cudf\n",
    "\n",
    "ddf = dask_cudf.read_csv(\n",
    "    \"data/*.csv\", # read all CSV files using the * wildcard\n",
    "    usecols=[\"STATION\", \"DlySum\", \"DATE\"],\n",
    "    dtype={\n",
    "        \"STATION\": \"object\",\n",
    "        \"DlySum\": np.uint32,\n",
    "        \"DATE\": \"datetime64[ns]\",\n",
    "    },\n",
    "    na_values=[\"-9999\"],\n",
    ")\n",
    "\n",
    "ddf = ddf.dropna() # method to remove missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36638dbe",
   "metadata": {},
   "source": [
    "For us, the above cell completed in less than **1000** milliseconds. All CSV files in **1000** milliseconds? Is that true!?\n",
    "Not really. What Dask is doing is building an [execution graph](https://tutorial.dask.org/01x_lazy.html).\n",
    "\n",
    "We can think of this as building the components to an assembly line. While we've optimized the process of calculating a result, we don't have the result yet. Let's get the mean rainfall data with the `mean` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"DlySum\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ca17d",
   "metadata": {},
   "source": [
    "Hmm, no results. In order to force a result, we can use the [`compute`](https://docs.rapids.ai/api/cudf/stable/dask-cudf.html) method. The result is stored as a cuDF object as opposed to a Dask computation node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_mean = ddf[\"DlySum\"].mean().compute()\n",
    "print('The mean is {:.2f}'.format(my_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce516be",
   "metadata": {},
   "source": [
    "Another way to force computation is [`persist`](https://docs.dask.org/en/stable/api.html#dask.persist). This keeps the results of the computation within the Dask cuDF environment. This is often used after loading data from disk memory in order to eliminate the relatively long process of reading files over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb183dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbc413",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_mean = ddf[\"DlySum\"].mean().compute()\n",
    "print('The mean is {:.2f}'.format(my_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb248af9",
   "metadata": {},
   "source": [
    "Great! Let's use this Dask dataframe to make a plot in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e0bf4",
   "metadata": {},
   "source": [
    "### Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3bc8a",
   "metadata": {},
   "source": [
    "Now that we have a way to quickly read and manipulate our data, let's make an interactive graph. [Plotly](https://plotly.com/) is a popular library for graphing and data dashboards.\n",
    "\n",
    "In order for Plotly to make a graph, data needs to be on the host, not the GPU.\n",
    "\n",
    "Let's try plotting our precipitation data over time. Since we have data going back to 1940, let's filter for the year 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by year\n",
    "mask = (ddf['DATE'] >= pd.Timestamp(2021, 1, 1)) & (ddf['DATE'] < pd.Timestamp(2022,1,1))\n",
    "ddf_year = ddf[mask]\n",
    "\n",
    "# Compute result and send to host\n",
    "df_year = ddf_year.compute().to_pandas()\n",
    "\n",
    "df_year.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef5c3a",
   "metadata": {},
   "source": [
    "To plot rainfall over time we're going to use theÂ [`Scatter Graph Object`](https://plotly.com/python-api-reference/generated/plotly.graph_objects.Scatter.html), which encompasses line charts, as well as scatter charts, text charts, and bubble charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e027e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure([go.Scatter(\n",
    "    x = df_year[\"DATE\"], \n",
    "    y = df_year[\"DlySum\"] # hundredths of an inch\n",
    "\n",
    ")])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50d3a2",
   "metadata": {},
   "source": [
    "There's also the `markers` mode and the [`update_layout`](https://plotly.com/python/reference/layout/) method which gives us even more options to style our figure. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15cad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Scatter(\n",
    "    x = df_year[\"DATE\"], \n",
    "    y = df_year[\"DlySum\"]/100, # inches\n",
    "    mode = 'markers'\n",
    "\n",
    ")])\n",
    "\n",
    "ft_colour = \"rgb(252, 3, 3)\" # define font colour\n",
    "\n",
    "fig.update_layout(\n",
    "        title = 'USA Precipitation 2021',\n",
    "        font_color = ft_colour\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec8436",
   "metadata": {},
   "source": [
    "#### Shut down the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd3eba",
   "metadata": {},
   "source": [
    "Explicitly shut down the kernel by running the cell below  - this prevents problems with using Dask in another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d6cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db567a34",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf71cae-6709-4c55-80d0-3b7c8a878116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_data_engineering (Conda)",
   "language": "python",
   "name": "sys_env_data_engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
